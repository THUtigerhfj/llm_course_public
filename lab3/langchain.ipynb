{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec3. LangChain\n",
    "\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n",
    "\n",
    "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n",
    "\n",
    "### Reference \n",
    "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "\n",
    "## 0. First thing first\n",
    "\n",
    "### 0.1 Dependencies and Keys\n",
    "  \n",
    "In addition to the Open AI keys, add the following keys to your .env file.\n",
    "\n",
    "- Serp api key:\n",
    "    ```\n",
    "    SERPAPI_API_KEY=\"YOURKEY\"\n",
    "    ```\n",
    "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n",
    "\n",
    "    After getting these two keys, set your keys as environment variables.\n",
    "- Langchain API key (for tracing)\n",
    "    ```\n",
    "    LANGCHAIN_TRACING_V2=\"true\"\n",
    "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n",
    "    ```\n",
    "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have installed these dependencies in your image\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL=\"qwen2.5-72b-instruct\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n",
    "OPENAI_BASE=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key abstractions in LangChain\n",
    "\n",
    "| Abstracted Components | Input Type                                | Output Type           |\n",
    "|-----------------------|-------------------------------------------|-----------------------|\n",
    "| Prompt                | Dictionary                                | PromptValue           |\n",
    "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n",
    "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The ChatModel\n",
    "\n",
    "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n",
    "\n",
    "`ChatModel` provides two methods to interact with the user:\n",
    "\n",
    "- `predict`: takes in a string, returns a string.\n",
    "- `predict_messages`: takes in a list of messages, returns a message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model=CHAT_MODEL,\n",
    "    base_url=OPENAI_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output utility to show the type and the content of the result\n",
    "def print_with_type(res):\n",
    "    print(f\"%s\" % (type(res)))\n",
    "    print(f\"%s\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four roles in LangChain, and you can define your own custom roles.\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user.\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A ChatMessage coming from the system.\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hello Xu Wei! Nice to meet you too! I\\'d be happy to tell you about LangChain.\\n\\nLangChain is a framework designed to help developers build applications that leverage large language models (LLMs) like me. It provides a set of tools and abstractions that make it easier to integrate LLMs into various applications, from chatbots and content generators to more complex systems that require natural language understanding and generation.\\n\\n### Key Features of LangChain:\\n1. **Modular Design**: LangChain is built with a modular architecture, allowing developers to mix and match different components to suit their specific needs. This includes different language models, data sources, and output formats.\\n\\n2. **Chain of Thought**: One of the core concepts in LangChain is the \"Chain of Thought\" (CoT) pattern. This involves breaking down complex tasks into a series of simpler steps, each of which can be handled by a language model. This approach helps in improving the accuracy and reliability of the model\\'s outputs.\\n\\n3. **Data Integration**: LangChain supports integration with various data sources, including databases, APIs, and file systems. This allows the language model to access and use external data to generate more informed and contextually relevant responses.\\n\\n4. **Customization**: Developers can customize the behavior of the language model by defining custom prompts, templates, and logic. This flexibility is crucial for tailoring the model to specific use cases and domains.\\n\\n5. **Security and Privacy**: LangChain includes features to help ensure the security and privacy of data. This is particularly important when dealing with sensitive information.\\n\\n6. **Community and Ecosystem**: LangChain has a growing community of developers and contributors. This community provides support, shares best practices, and develops additional tools and plugins that extend the functionality of the framework.\\n\\n### Use Cases:\\n- **Chatbots**: Building conversational agents that can engage in natural language conversations with users.\\n- **Content Generation**: Generating articles, blog posts, and other types of content.\\n- **Code Generation**: Assisting developers by generating code snippets or entire functions.\\n- **Data Analysis**: Analyzing and summarizing large datasets.\\n- **Personal Assistants**: Creating personal assistants that can help with tasks like scheduling, reminders, and information retrieval.\\n\\n### Getting Started:\\nIf you\\'re interested in using LangChain, you can start by checking out the official documentation and tutorials. The documentation provides detailed guides on how to set up the framework, integrate different language models, and build your first application.\\n\\nI hope this gives you a good overview of LangChain! If you have any more questions or need further details, feel free to ask!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 530, 'prompt_tokens': 50, 'total_tokens': 580, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-e0b3e743-9df7-4e95-8b03-2b46dc5c943e-0' usage_metadata={'input_tokens': 50, 'output_tokens': 530, 'total_tokens': 580, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n",
    "\n",
    "messages = []\n",
    "messages.append(\n",
    "    HumanMessage(content=qtext)  # construct a human message\n",
    "    )\n",
    "res = chat_model.invoke(messages)  # invoke the chat model\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "messages.append(res)  # append the result to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructors are tedious to use, and you can use the following more friendly API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"I'm just here to help you out! What can I assist you with today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 50, 'total_tokens': 68, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-37c06fb1-d3c7-4f24-baa2-d53d93a0b5bc-0' usage_metadata={'input_tokens': 50, 'output_tokens': 18, 'total_tokens': 68, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# a simpler way to manage messages\n",
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"nothing much, you?\")\n",
    "\n",
    "res = chat_model.invoke(history.messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"LangChain has a wide range of applications across various industries and use cases. Here are some of the key areas where LangChain can be particularly useful:\\n\\n### 1. **Chatbots and Virtual Assistants**\\n- **Customer Support**: Building chatbots that can handle customer inquiries, provide product information, and assist with troubleshooting.\\n- **Personal Assistants**: Creating virtual assistants that can manage schedules, set reminders, and perform tasks on behalf of users.\\n\\n### 2. **Content Generation**\\n- **Article Writing**: Generating articles, blog posts, and news updates.\\n- **Social Media Posts**: Creating engaging social media content.\\n- **Marketing Copy**: Writing marketing materials, product descriptions, and ad copy.\\n\\n### 3. **Code Generation and Assistance**\\n- **Code Snippets**: Generating code snippets for specific tasks or problems.\\n- **Documentation**: Writing documentation for code and APIs.\\n- **Code Review**: Providing suggestions and feedback on code quality.\\n\\n### 4. **Data Analysis and Reporting**\\n- **Data Summarization**: Summarizing large datasets and providing insights.\\n- **Report Generation**: Creating detailed reports and visualizations.\\n- **Predictive Analytics**: Using language models to make predictions based on historical data.\\n\\n### 5. **Educational Tools**\\n- **Tutoring Systems**: Building systems that can provide personalized tutoring and feedback.\\n- **Content Creation**: Generating educational content, quizzes, and exercises.\\n- **Language Learning**: Creating tools to help users learn new languages.\\n\\n### 6. **E-commerce and Retail**\\n- **Product Recommendations**: Providing personalized product recommendations based on user preferences and behavior.\\n- **Customer Reviews**: Generating and summarizing customer reviews.\\n- **Inventory Management**: Assisting with inventory management and stock updates.\\n\\n### 7. **Healthcare**\\n- **Patient Engagement**: Building chatbots that can provide health information, appointment reminders, and symptom checkers.\\n- **Medical Documentation**: Assisting healthcare professionals in writing and summarizing medical records.\\n- **Research Assistance**: Helping researchers by summarizing and analyzing medical literature.\\n\\n### 8. **Legal and Compliance**\\n- **Contract Review**: Assisting in the review and summarization of legal documents.\\n- **Compliance Checking**: Ensuring that documents and processes comply with regulations.\\n- **Legal Research**: Providing assistance in legal research and case analysis.\\n\\n### 9. **Financial Services**\\n- **Financial Advice**: Providing personalized financial advice and investment recommendations.\\n- **Risk Assessment**: Analyzing and assessing financial risks.\\n- **Report Generation**: Creating financial reports and summaries.\\n\\n### 10. **Gaming and Entertainment**\\n- **NPC Dialogue**: Generating dialogue for non-player characters (NPCs) in video games.\\n- **Story Generation**: Creating dynamic and engaging storylines for games and interactive media.\\n- **Content Moderation**: Assisting in the moderation of user-generated content.\\n\\n### 11. **Research and Development**\\n- **Literature Review**: Summarizing and analyzing research papers and articles.\\n- **Hypothesis Generation**: Assisting in the generation of research hypotheses.\\n- **Data Analysis**: Providing insights and analysis for research projects.\\n\\n### 12. **Customer Experience**\\n- **Feedback Analysis**: Analyzing customer feedback and reviews to identify trends and areas for improvement.\\n- **Survey Creation**: Generating and analyzing surveys to gather customer insights.\\n- **Personalization**: Tailoring customer experiences based on individual preferences and behavior.\\n\\n### 13. **Human Resources**\\n- **Recruitment**: Assisting in the recruitment process by generating job descriptions, screening resumes, and conducting initial interviews.\\n- **Employee Onboarding**: Creating onboarding materials and providing support to new employees.\\n- **Performance Reviews**: Assisting in the creation and analysis of performance reviews.\\n\\n### 14. **Marketing and Sales**\\n- **Lead Generation**: Generating leads and qualifying potential customers.\\n- **Sales Support**: Providing sales teams with product information, customer insights, and sales strategies.\\n- **Campaign Analysis**: Analyzing marketing campaigns and providing insights for optimization.\\n\\n### 15. **Customer Service**\\n- **Ticket Management**: Assisting in the management of customer support tickets.\\n- **FAQ Generation**: Creating and maintaining frequently asked questions (FAQs).\\n- **Customer Feedback**: Collecting and analyzing customer feedback to improve services.\\n\\n### Conclusion\\nLangChain's flexibility and modular design make it a powerful tool for a wide range of applications. Whether you're building a chatbot, generating content, or analyzing data, LangChain can help you leverage the power of large language models to create more efficient and effective solutions. If you have a specific use case in mind, feel free to share more details, and I can provide more tailored advice!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 948, 'prompt_tokens': 594, 'total_tokens': 1542, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-9498d925-a701-4240-a894-f3261d9e405c-0' usage_metadata={'input_tokens': 594, 'output_tokens': 948, 'total_tokens': 1542, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# remembering the chat history and context\n",
    "\n",
    "qtext = \"what is its application?\"\n",
    "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n",
    "messages.append(res)  ## remembers the histroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt templates\n",
    "\n",
    "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n",
    "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n",
    "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n",
    "\n",
    "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Simple template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes candies?\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "input_prompt = prompt.format(product=\"candies\")\n",
    "\n",
    "print_with_type(input_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Chat Template (a list of temlates in a chat prompt template)\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# format chat message prompt\n",
    "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
    "\n",
    "print_with_type(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Using template in the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-4704bdb4-7bdf-41e4-8c26-95bc377e12f2-0' usage_metadata={'input_tokens': 29, 'output_tokens': 5, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# format messages with PromptTemplate with translator as an example\n",
    "messages = []\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n",
    "print_with_type(chat_input)\n",
    "print_with_type(chat_model.invoke(chat_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-178e30f5-a356-49c1-b8c8-3909a9603f24-0' usage_metadata={'input_tokens': 29, 'output_tokens': 5, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "messages = chat_input + messages  ## the system message must be at the beginning\n",
    "print_with_type(messages)\n",
    "\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chaining Components together\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n",
    "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n",
    "\n",
    "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n",
    "\n",
    "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n",
    "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n",
    "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "- LangSmith Tracing Integration: (for debugging, see below).\n",
    "\n",
    "In this lab, we only demonstrate the simplest functional chainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='{\"这是输入到聊天模型链和聊天提示的内容。\"}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 39, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-09e8eb0a-f24e-4770-9892-14f5ebfe3941-0' usage_metadata={'input_tokens': 39, 'output_tokens': 14, 'total_tokens': 53, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# More abstractions: bundling prompt and the chat_model into a chain\n",
    "\n",
    "translate_chain = chat_prompt | chat_model\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = translate_chain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Output parser\n",
    "\n",
    "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n",
    "\n",
    "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n",
    "\n",
    "LangChain provides several commonly-used output parsers, including:\n",
    "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n",
    "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n",
    "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Simple output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{\"这是输入到聊天模型链和聊天提示的内容。\"}\n"
     ]
    }
   ],
   "source": [
    "# a simple output parser\n",
    "# StdOutParser converts the chat message to a string.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "stdoutchain = chat_prompt | chat_model | output_parser\n",
    "\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out=stdoutchain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Advanced output parsers: from Results to a Python Object\n",
    "Here we demonstrate the powerful Json Outputparser as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'A Survey of Deep Learning System Optimization, Deep Learning-based Text Representation: A Survey, Efficient and Effective Algorithms for Training Quantized Neural Networks, and more'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "professor_chain = prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of the Professor\", \"title\": \"Name\", \"type\": \"string\"}, \"publications\": {\"description\": \"the string of the professor\\'s publications, separated by comma\", \"title\": \"Publications\", \"type\": \"string\"}}, \"required\": [\"name\", \"publications\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}'\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# see how langchain organizes the input to construct the result.\n",
    "# you can do so by printing the input of the chat_model.\n",
    "\n",
    "print_with_type(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n",
    "\n",
    "This is because the model has no knowlege about Prof. Wei Xu.  \n",
    "\n",
    "We will now demonstrate how to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding more contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Allowing the model to search the web: Retrievers\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n",
    "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n",
    "\n",
    "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n",
    "\n",
    "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n",
    "\n",
    "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'title': 'Nvidia Stock To $60?', 'link': 'https://www.forbes.com/sites/greatspeculations/2025/03/14/nvidia-stock-to-60/', 'source': 'Forbes', 'date': '18 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24b43c2f81805cf6b5b3be5815be8f76320.jpeg'}, {'title': \"Nvidia Stock Held Up Better Than Mag 7 Peers During Thursday's Rout—Watch These Key Levels\", 'link': 'https://www.investopedia.com/nvidia-stock-held-up-better-than-mag-7-peers-during-thursday-rout-watch-these-key-levels-11696055', 'source': 'Investopedia', 'date': '1 day ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24b4c7b0aaf8781d047769a910f60ccdbb5.jpeg'}, {'title': 'Stocks making the biggest moves midday: Ulta, Nvidia, Chipotle, Peloton and more', 'link': 'https://www.cnbc.com/2025/03/14/stocks-making-the-biggest-moves-midday-ulta-nvidia-chipotle-peloton-and-more.html', 'source': 'CNBC', 'date': '11 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24bc3f521adf0ee9f31107994e50c988a68.jpeg'}, {'title': 'Quantum Computing Stocks Jump Ahead Of Nvidia GTC Conference Next Week', 'link': 'https://www.investors.com/news/technology/quantum-computing-stocks-jump-nvidia-gtc-conference/', 'source': \"Investor's Business Daily\", 'date': '8 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24bfae7f5d60d411131c3c58ec69e5546a1.jpeg'}, {'title': 'What to expect at the NVIDIA GTC 2025 keynote with CEO Jensen Huang', 'link': 'https://www.engadget.com/computing/what-to-expect-at-the-nvidia-gtc-2025-keynote-with-ceo-jensen-huang-183038999.html', 'source': 'Engadget', 'date': '1 day ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24babd727336341ea3db8d3f4b7e073af98.jpeg'}, {'title': 'Nvidia Could Be Joining Intel After All (NASDAQ:INTC)', 'link': 'https://seekingalpha.com/article/4767396-nvidia-could-be-joining-intel-after-all', 'source': 'Seeking Alpha', 'date': '1 day ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24b66fb3dbf7d1dd2bb5aaa3616b9d0b852.jpeg'}, {'title': \"Nvidia's Huang, critics were wrong about quantum computing: D-Wave CEO\", 'link': 'https://finance.yahoo.com/video/nvidias-huang-critics-were-wrong-170000181.html', 'source': 'Yahoo Finance', 'date': '1 day ago', 'thumbnail': 'https://serpapi.com/searches/67d4f372fa0b39239e2770f4/images/1facaa3a5b60e24b8138a12753d5821b3cc4677d8b5018de.jpeg', 'label': 'Video'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the search API\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "results = search.run(\"Nvidia\")\n",
    "print_with_type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the search and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class News(BaseModel):\n",
    "    title: str = Field(description=\"title of the news\")\n",
    "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_chain.invoke(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging and Logging\n",
    "\n",
    "#### 2.2.1 The debug mode\n",
    "\n",
    "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n",
    "\n",
    "1. The exact prompts being sent to the LLM\n",
    "2. The raw responses received from the LLM\n",
    "3. The execution flow of chains and agents\n",
    "4. Any intermediate steps and tool calls\n",
    "\n",
    "This is extremely useful for:\n",
    "- Troubleshooting unexpected outputs\n",
    "- Understanding how your chains are processing data\n",
    "- Optimizing prompts\n",
    "- Identifying errors in your chain's logic\n",
    "\n",
    "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [2.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [2.59s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\nNVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the company nvidia, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.21s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 70,\n",
      "                \"prompt_tokens\": 294,\n",
      "                \"total_tokens\": 364,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1593ade2-12cd-4471-a29c-6be8246512b2-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 294,\n",
      "              \"output_tokens\": 70,\n",
      "              \"total_tokens\": 364,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 70,\n",
      "      \"prompt_tokens\": 294,\n",
      "      \"total_tokens\": 364,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [11ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.82s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"}\n"
     ]
    }
   ],
   "source": [
    "# Debugging and logging: debug mode\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"}\n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Tracing with LangSmith\n",
    "\n",
    "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n",
    "\n",
    "1. **Tracing**: Visualize the execution flow of your chains and agents\n",
    "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n",
    "3. **Evaluation**: Measure and compare model performance\n",
    "4. **Monitoring**: Track usage and performance in production\n",
    " \n",
    "To use LangSmith tracing:\n",
    "Set environment variables (in your .env file):\n",
    "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n",
    "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n",
    "\n",
    "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA: The Engine of Global AI Infrastructure',\n",
       " 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    return websearch_chain.invoke(user_input)\n",
    "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "# Out:  Hello there! How can I assist you today? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your task ####\n",
    "# go to the langsmith webpage and observe the traces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![traces](images/traces.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': '20+ research papers in leading venues, 17 research works with 833 citations, including: Antiproliferative activities of the second-generation antipsychotic drug sertindole against breast'}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# retrieve the information and fix the query results about Prof. Xu, \n",
    "# generating the correct Professor object.\n",
    "# Note that you do not have to get a perfect answer from the LLM in this lab.  \n",
    "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n",
    "# class News(BaseModel):\n",
    "#     title: str = Field(description=\"title of the news\")\n",
    "#     brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  \n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "professor_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "res = professor_chain.invoke(\"tell me about professor Wei Xu at Tsinghua University, including his specific publications.\")\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# analyze the answer, if the answer is not correct, write down some comments \n",
    "# about why the answer is not correct. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is of course not correct, but it leads to an interesting fact that *there's another Prof. Wei Xu at Tsinghua University and he has even published a paper on Nature*. Our web search actually finds the information about this Prof. Wei Xu.\n",
    "\n",
    "![publication](images/publication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smarter workflow: Agents\n",
    "\n",
    "\n",
    "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n",
    "\n",
    "LangChain provides several frameworks for building agents:\n",
    "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n",
    "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n",
    "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n",
    "4. **Structured output**: Helps parse and validate agent responses\n",
    "\n",
    "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function-calling: Letter r's in straberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are two \\'r\\'s in the word \"strawberry.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 40, 'total_tokens': 55, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}}, id='run-23139c62-4ac7-465d-a01f-f9b66d1299d2-0', usage_metadata={'input_tokens': 40, 'output_tokens': 15, 'total_tokens': 55, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"how many r's are there in the word strawberry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[StructuredTool(name='get_letter_count', description='Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)', args_schema=<class 'langchain_core.utils.pydantic.get_letter_count'>, func=<function get_letter_count at 0x7fbe142dfd00>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n",
    "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n",
    "# to fit two parameters, we need a clear format instruction for the input.\n",
    "@tool\n",
    "def get_letter_count(query: str) -> int:\n",
    "    \"\"\"Returns the number of a specific letter in a word. \n",
    "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n",
    "    word, letter = query.split(',')\n",
    "    return word.count(letter)\n",
    "\n",
    "print(get_letter_count.invoke(\"strawberry,r\"))\n",
    "\n",
    "tools = [ get_letter_count ]\n",
    "\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at counting letters in words.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.36s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 37,\n",
      "                \"prompt_tokens\": 201,\n",
      "                \"total_tokens\": 238,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e25b4cd2-bca6-4f1f-959c-7a151f58aa13-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 201,\n",
      "              \"output_tokens\": 37,\n",
      "              \"total_tokens\": 238,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 37,\n",
      "      \"prompt_tokens\": 201,\n",
      "      \"total_tokens\": 238,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.37s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] Entering Tool run with input:\n",
      "\u001b[0m\"strawberry,r\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"3\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [911ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 23,\n",
      "                \"prompt_tokens\": 242,\n",
      "                \"total_tokens\": 265,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c7b30ffc-8751-42b9-a25d-97741c96a3c1-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 242,\n",
      "              \"output_tokens\": 23,\n",
      "              \"total_tokens\": 265,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 23,\n",
      "      \"prompt_tokens\": 242,\n",
      "      \"total_tokens\": 265,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [922ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are 3 'r's in the word strawberry.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'help me sort the numbers 4, 3, 5, 1, 2.', 'output': 'The sorted numbers are [1, 2, 3, 4, 5].'}\n"
     ]
    }
   ],
   "source": [
    "#### your task ####\n",
    "# implement a tool that sort an array of numbers (packed as a comma separated string)\n",
    "# then ask the agent to use the tool to sort an input array. \n",
    "from langchain.agents import tool\n",
    "import ast\n",
    "\n",
    "@tool\n",
    "def sort_the_array(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Sort the given array of numbers and return the sorted array.\n",
    "    The input should be in the format of string which is a python list of the array. (e.g., \"[4, 3, 5, 1, 2]\")\n",
    "    \"\"\"\n",
    "    return sorted(ast.literal_eval(query))\n",
    "\n",
    "tools = [ sort_the_array ]\n",
    "# print(sort_the_array.invoke(\"[4, 3, 5, 1, 2]\"))\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad sorting array of numbers.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "response = agent_chain.invoke({\"input\": \"help me sort the numbers 4, 3, 5, 1, 2.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create an auto-web-search AI Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the following example about the built-in web-search tool and understand the code. \n",
    "\n",
    "\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for recent news from Tsinghua University to provide an updated answer.\n",
      "Action: Search\n",
      "Action Input: \"Tsinghua University news last week\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"LATEST NEWS · \\u200bResearch team from Department of Computer Science and Technology wins Best Paper Award at FAST 2025 · Bai Chong-En: China's economic future and ...\", '\\u200bResearch teams from Tsinghua University and Beijing Tongren Hospital discover new immunological mechanisms driving the recurrence of chronic rhinosinusitis...', 'The latest news, analysis and opinion on Tsinghua University. In-depth analysis, industry insights and expert opinion.', 'LATEST NEWS · Mar25. Spring in Tsinghua · Mar18. Exhibition of Indonesian modern, contemporary art opens in Beijing · Mar17. Tsinghua hosts job fair for 2021 ...', 'LATEST NEWS · May26. Pritzker-Tsinghua Fellowship Program Inaugurated · May20. Pennsylvania State University President Visits Tsinghua · May16. Tsinghua Ranks ...', \"China's premier cheers on makers in a written reply to Tsinghua students. Chinese Premier Li Keqiang has lauded the younger generation's spirit of ...\", \"In a recent year, 41 percent of the university's non-Chinese students were undergraduates, 29 percent were pursuing master's degrees, 9 percent were doctoral ...\", 'HEADLINES · Multi-stakeholders from 6 Continents Co-initiate the Global Youth Climate Week · HEADLINES · The 2nd World Health Forum Opens · HEADLINES · \\u200bWhere ...', 'In Depth: Tsinghua Program Shows the Challenges of Nurturing World-Class Mathematicians - Lured by the possibility of direct admission into ...', \"I'm in a Tsinghua University program this weekend with a bunch of students from across Asia, and the one phrase that repeatedly keeps coming ...\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide a variety of recent news items from Tsinghua University. I will summarize the most relevant ones.\n",
      "Final Answer: Within the last week, some of the notable news from Tsinghua University includes:\n",
      "- A research team from the Department of Computer Science and Technology won the Best Paper Award at FAST 2025.\n",
      "- Researchers from Tsinghua University and Beijing Tongren Hospital have discovered new immunological mechanisms driving the recurrence of chronic rhinosinusitis.\n",
      "- The university hosted a job fair for the 2021 graduates.\n",
      "- An exhibition of Indonesian modern and contemporary art opened in Beijing, with Tsinghua University's involvement.\n",
      "- The Pritzker-Tsinghua Fellowship Program was inaugurated, highlighting the university's international collaborations.\n",
      "- Chinese Premier Li Keqiang lauded the younger generation's spirit of innovation in a written reply to Tsinghua students, encouraging them to continue their innovative efforts.\n",
      "\n",
      "These highlights showcase the diverse activities and achievements at Tsinghua University, ranging from academic research to cultural exchanges and government interactions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the news from tsinghua university within last week?',\n",
       " 'output': \"Within the last week, some of the notable news from Tsinghua University includes:\\n- A research team from the Department of Computer Science and Technology won the Best Paper Award at FAST 2025.\\n- Researchers from Tsinghua University and Beijing Tongren Hospital have discovered new immunological mechanisms driving the recurrence of chronic rhinosinusitis.\\n- The university hosted a job fair for the 2021 graduates.\\n- An exhibition of Indonesian modern and contemporary art opened in Beijing, with Tsinghua University's involvement.\\n- The Pritzker-Tsinghua Fellowship Program was inaugurated, highlighting the university's international collaborations.\\n- Chinese Premier Li Keqiang lauded the younger generation's spirit of innovation in a written reply to Tsinghua students, encouraging them to continue their innovative efforts.\\n\\nThese highlights showcase the diverse activities and achievements at Tsinghua University, ranging from academic research to cultural exchanges and government interactions.\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about professor Wei Xu at IIIS, Tsinghua University, including his specific publications.',\n",
       " 'output': 'Professor Wei Xu is a faculty member at the Institute for Interdisciplinary Information Sciences (IIIS) and CollegeAI at Tsinghua University. He has a significant number of citations, indicating a strong research presence in the field of Computer Science. His research interests include high performance computing, scientific computing, parallel I/O, climate system modeling, and uncertainty. For a detailed list of his publications, you should visit his Google Scholar profile, which can be found by searching for \"Wei Xu Tsinghua University\" on Google Scholar.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# use the web search tool to find out about \n",
    "# prof. wei xu and his publication list.  \n",
    "# Compare the results with the previous implementation. is it better or worse?\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=False)\n",
    "\n",
    "agent_chain.invoke(\"tell me about professor Wei Xu at IIIS, Tsinghua University, including his specific publications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use one of the built-in tool for langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " echo \"Hello World! The current time is $(date)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " echo \"Hello World! The current time is $(date)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " echo \"Hello World!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " echo \"Hello World!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Print out \"Hello World!\" and the current time.',\n",
       " 'output': 'The expected output should be \"Hello World! The current time is [current time]\", but there seems to be an issue with the terminal execution environment. If you can run the command `echo \"Hello World! The current time is $(date)\"` in a standard Linux terminal, it will produce the desired output.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# use a built-in tool in langchain to complete a task of your choice. \n",
    "# in the comment, please describe the task and the tool you used. \n",
    "# optional: try to use more than one tool in the same agent\n",
    "\n",
    "# Task: Let the LLM use perform some easy shell commands.\n",
    "# Tool: ShellTool\n",
    "\n",
    "\n",
    "from langchain_community.tools import ShellTool\n",
    "shell_tool = ShellTool()\n",
    "# print(shell_tool.run({\"commands\": [\"echo 'Hello World!'\", \"time\"]}))\n",
    "\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users to perform commands in Shell: {commands}\",\n",
    "        ),\n",
    "        (\"user\", \"{commands}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent([shell_tool], chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=False)\n",
    "\n",
    "agent_chain.invoke(\"Print out \\\"Hello World!\\\" and the current time.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
