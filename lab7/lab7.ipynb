{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: LLM API server and Web interfaces\n",
    "\n",
    "In this lecture, you will learn how to serve modern large models on Linux servers with easy-to-use user interface. We will be using Python as our main programming language, and we do not require knowledge about front-end language such as Javascript or CSS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Calling Web Service APIs\n",
    "\n",
    "In this experiment, we'll equip you with the basic knowledge and practical skills to start making powerful HTTP requests in Python. We'll cover GET and POST methods, and explore JSON data exchange. So, buckle up, let's code!\n",
    "\n",
    "First, we will need `requests` library. It should be installed by default in your Python environment, but if you don't have it, you can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Basic `GET`\n",
    "\n",
    "GET retrieves information from a specific web address (URL). Parameters are passed either in the path itself or as a query parameter (after ? in the URL).\n",
    "\n",
    "Let's try the GET method to retrieve a random joke!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "\n",
      "--- Response Text ---\n",
      "{\"categories\":[\"history\"],\"created_at\":\"2020-01-05 13:42:19.576875\",\"icon_url\":\"https://api.chucknorris.io/img/avatar/chuck-norris.png\",\"id\":\"rqcvwdgqq6amwony3nngba\",\"updated_at\":\"2020-01-05 13:42:19.576875\",\"url\":\"https://api.chucknorris.io/jokes/rqcvwdgqq6amwony3nngba\",\"value\":\"In the Words of Julius Caesar, \\\"Veni, Vidi, Vici, Chuck Norris\\\". Translation: I came, I saw, and I was roundhouse-kicked inthe face by Chuck Norris.\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Target URL\n",
    "url = \"https://api.chucknorris.io/jokes/random\"\n",
    "\n",
    "# Send a GET request and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code (2XX means success)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "\n",
    "# Access the response content (raw bytes)\n",
    "content = response.content\n",
    "\n",
    "# Decode the content to text (may differ depending on API)\n",
    "text = content.decode(response.encoding)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\n--- Response Text ---\")\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Playing with JSON\n",
    "\n",
    "Many APIs and websites return data in the JSON format, a structured way to organize information. We can easily convert this JSON string to a Python dictionary for easy access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categories': ['history'],\n",
      " 'created_at': '2020-01-05 13:42:19.576875',\n",
      " 'icon_url': 'https://api.chucknorris.io/img/avatar/chuck-norris.png',\n",
      " 'id': 'rqcvwdgqq6amwony3nngba',\n",
      " 'updated_at': '2020-01-05 13:42:19.576875',\n",
      " 'url': 'https://api.chucknorris.io/jokes/rqcvwdgqq6amwony3nngba',\n",
      " 'value': 'In the Words of Julius Caesar, \"Veni, Vidi, Vici, Chuck Norris\". '\n",
      "          'Translation: I came, I saw, and I was roundhouse-kicked inthe face '\n",
      "          'by Chuck Norris.'}\n",
      "{\"categories\": [\"history\"], \"created_at\": \"2020-01-05 13:42:19.576875\", \"icon_url\": \"https://api.chucknorris.io/img/avatar/chuck-norris.png\", \"id\": \"rqcvwdgqq6amwony3nngba\", \"updated_at\": \"2020-01-05 13:42:19.576875\", \"url\": \"https://api.chucknorris.io/jokes/rqcvwdgqq6amwony3nngba\", \"value\": \"In the Words of Julius Caesar, \\\"Veni, Vidi, Vici, Chuck Norris\\\". Translation: I came, I saw, and I was roundhouse-kicked inthe face by Chuck Norris.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "dict = json.loads(text)\n",
    "pprint(dict)\n",
    "\n",
    "encoded_json = json.dumps(dict)\n",
    "print(encoded_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Moving on to POST Requests\n",
    "\n",
    "While GET requests fetch data, POST requests send information to a server, like submitting a form. We'll be using a dummy API that echos the data we sent as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"30\", \n",
      "    \"name\": \"John Doe\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, zstd\", \n",
      "    \"Content-Length\": \"20\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6825da2e-680a85662195af3c3a9ea8fe\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"method\": \"POST\", \n",
      "  \"origin\": \"114.253.255.159\", \n",
      "  \"url\": \"https://httpbin.org/anything\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define URL and data\n",
    "url = \"https://httpbin.org/anything\"\n",
    "data = {\"name\": \"John Doe\", \"age\": 30}  # a python dictionary\n",
    "\n",
    "# Send POST request with data\n",
    "response = requests.post(url, data=data) # data is automatically encoded to json\n",
    "\n",
    "# Check status code and print response\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sent data is actually received by the server (`form` shows the exactly the same data we sent).\n",
    "\n",
    "This is just the tip of the iceberg! Now you have seen how we can utilize the existing web service. In the remaining experiments, you will be building your own API server and web service with a nice user interface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Creating an API server using FastAPI\n",
    "\n",
    "Most of you should have experienced the LLM APIs we provided, which allows your program accessing the power of large language models. Here we will guide you to build your own LLM service, using the `fastapi` library of Python.\n",
    "\n",
    "`fastapi` takes care of the job of launching a web server and serve the API calls. You only need to define a function that takes the input data from the request to produce output. `fastapi` will handle the rest things for you.\n",
    "\n",
    "First, install the dependency of `fastapi` if needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basics on FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install uvicorn fastapi websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/fastapi_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/fastapi_example.py\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "## path parameters\n",
    "@app.get('/g/{data}')\n",
    "async def process_data(data: str):\n",
    "    return f'Processed {data} by FastAPI!'\n",
    "\n",
    "fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n",
    "# Query parameters\n",
    "@app.get(\"/items/\")\n",
    "async def read_item(skip: int = 0, limit: int = 10):\n",
    "    return fake_items_db[skip : skip + limit]\n",
    "\n",
    "\n",
    "## The data model\n",
    "from typing import List\n",
    "class Sale(BaseModel):\n",
    "    day: int\n",
    "    price: float\n",
    "    \n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    inventory: int | None = 10\n",
    "    sales: List[Sale] = []\n",
    "\n",
    "# Getting Parameters from Request\n",
    "@app.post(\"/post\")\n",
    "async def create_item(item: Item):\n",
    "    return f'Hello {item.name}, {item.inventory} in stock, sold {len(item.sales)} items'\n",
    "\n",
    "# The main() function is the entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the following command in your terminal to start the server\n",
    "## python /tmp/fastapi_example.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\"Processed hello by FastAPI!\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can visit your web service at:\n",
    "\n",
    "response = requests.get('http://localhost:54223/g/hello')\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'[{\"item_name\":\"Baz\"}]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the query parameter\n",
    "response = requests.get('http://localhost:54223/items?skip=2&limit=3')\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let the magic happen.  \n",
    "# Set port forwarding in your VSCode devcontainer to forward port 54223 to your local machine\n",
    "# Then visit `http://localhost:54223/g/hello` in your browser, you will be able to see the return string in the browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "\"Hello Apple, 33 in stock, sold 2 items\"\n"
     ]
    }
   ],
   "source": [
    "# Also test the POST processing, with a complex data structure as input\n",
    "\n",
    "url = \"http://localhost:54223/post\"\n",
    "data = { \"name\": \"Apple\", \n",
    "         \"inventory\": 33, \n",
    "         \"sales\": [{\"day\": 0, \"price\": 3.4}, {\"day\": 1, \"price\": 3.3}]\n",
    "         }\n",
    "encoded = json.dumps(data).encode(\"utf-8\")\n",
    "response = requests.post(url, data=encoded)  # the parameters should be encoded as JSON\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another FastAPI magic: automatic document generation\n",
    "# Visit http://localhost:54223/docs in your browser to see the API documentation\n",
    "# (Assuming that you have your port forwarding set up correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating an API to serve local LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recall how you run a local LLM.  The following scripts starts a Phi-4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/local_llm.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/local_llm.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.6,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n",
    "    else:\n",
    "        messages = history\n",
    "    if user_prompt:\n",
    "        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        messages.extend(prompt_msg)\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output\n",
    "\n",
    "## The main function is the entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True,\n",
    "                                             )\n",
    "    resp = chat_resp(model, tokenizer, \"What is the meaning of life?\")\n",
    "    print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first verify that you can run LLM locally correctly (it should print out the results, despite of lots of warnings.)\n",
    "## python /tmp/local_llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/llm_api.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/llm_api.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "         \n",
    "from fastapi import FastAPI, Request, Query\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.6,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n",
    "    else:\n",
    "        messages = history\n",
    "    if user_prompt:\n",
    "        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        messages.extend(prompt_msg)\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output\n",
    "\n",
    "#### Your Task ####\n",
    "## Implement a GET handler that takes in a single string as prompt from user,\n",
    "## and return the response as a single string.\n",
    "#### End Task #### \n",
    "@app.get(\"/run\")\n",
    "async def run_get(q: str):\n",
    "    output = chat_resp(model, tokenizer, user_prompt=q)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "#### Your Task ####\n",
    "## Implement a POST handler that takes in a single string and a history\n",
    "## and return the response as a single string.\n",
    "#### End Task ####\n",
    "class Input(BaseModel):\n",
    "    Prompt: str\n",
    "    History: list\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def run_post(input: Input):\n",
    "    output = chat_resp(model, tokenizer, user_prompt=input.Prompt, history=input.History)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "#### Your Task ####\n",
    "## The main function is the entry point of the script, you should load the model\n",
    "## and then start the FastAPI server.\n",
    "#### End Task ####\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True,\n",
    "                                             )\n",
    "    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the following command in your terminal to start the server\n",
    "## python /tmp/llm_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:54223/run?q=%E4%B8%AD%E5%9B%BD%E7%9A%84%E9%A6%96%E9%83%BD%E6%98%AF%E5%93%AA%E9%87%8C%EF%BC%9F\n",
      "Status code: 200\n",
      "\"中国的首都是北京。\"\n"
     ]
    }
   ],
   "source": [
    "## Run a single query to test the API, using GET\n",
    "\n",
    "import urllib.parse\n",
    "params = {\"q\": \"中国的首都是哪里？\"}\n",
    "prompt_url = urllib.parse.urlencode(params)\n",
    "url = f'http://localhost:54223/run?%s' % prompt_url\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.content.decode(response.encoding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "\"刘可翰是男生。根据您提供的初始信息，刘可翰被描述为女生。然而，姓名“刘可翰”通常是男性的姓名。如果您希望确认刘可翰的性别，最好咨询该个人或检查相关背景信息。\"\n"
     ]
    }
   ],
   "source": [
    "#### Your Task ####\n",
    "## Run a LLM single line query with POST, and add chat history (history stored on the client side only)\n",
    "url = \"http://localhost:54223/chat\"\n",
    "data = { \"Prompt\": \"刘可翰是男生还是女生？\", \n",
    "         \"History\": [{\"role\": \"user\", \"content\": \"刘可翰是女生。\"}]\n",
    "}\n",
    "encoded = json.dumps(data).encode(\"utf-8\")\n",
    "response = requests.post(url, data=encoded)  # the parameters should be encoded as JSON\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Creating OpenAI-Compatible API server using vLLM\n",
    "\n",
    "In the previous section, we have created a simple API server using FastAPI. However, the OpenAI-like API has been de facto standard for LLM services. Manual implementation of the OpenAI API is tedious. Luckily, there are many open-source frameworks that provide OpenAI-compatible APIs. In this section, we will use vLLM to create an OpenAI-compatible API server.\n",
    "\n",
    "vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It uses a novel GPU memory management technique called \"PagedAttention\" to enable efficient inference of large models.\n",
    "\n",
    "vLLM has two modess: Offline Inference and OpenAI-Compatible Server:\n",
    "- **Offline Inference**: This mode is just like the huggingface transformers library. You can load a model and run inference by using vllm as a library.\n",
    "- **OpenAI-Compatible Server**: This mode provides endpoints compatible with the OpenAI API, allowing you to run your own LLMs with a similar interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Offline Inference\n",
    "\n",
    "The offline API is based on the LLM class. To initialize the vLLM engine, create a new instance of LLM and specify the model to run.\n",
    "\n",
    "The LLM class provides various methods for offline inference. See Engine Arguments for a list of options when initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 21:12:24 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-15 21:12:35 [config.py:600] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-15 21:12:35 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-15 21:12:36 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/ssdshare/share/model/Qwen3-0.6B-Base', speculative_config=None, tokenizer='/ssdshare/share/model/Qwen3-0.6B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/ssdshare/share/model/Qwen3-0.6B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-15 21:12:37 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f04f5489990>\n",
      "INFO 05-15 21:12:38 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-15 21:12:38 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 21:12:38 [gpu_model_runner.py:1258] Starting to load model /ssdshare/share/model/Qwen3-0.6B-Base...\n",
      "WARNING 05-15 21:12:38 [utils.py:76] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "INFO 05-15 21:12:38 [transformers.py:118] Using Transformers backend.\n",
      "WARNING 05-15 21:12:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16a3983742749fb9f864103158fba9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 21:12:45 [loader.py:447] Loading weights took 6.58 seconds\n",
      "INFO 05-15 21:12:45 [gpu_model_runner.py:1273] Model loading took 1.1103 GiB and 7.318280 seconds\n",
      "INFO 05-15 21:12:54 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/ba6cb5e81d/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-15 21:12:54 [backends.py:426] Dynamo bytecode transform time: 8.50 s\n",
      "INFO 05-15 21:13:02 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 05-15 21:13:32 [backends.py:144] Compiling a graph for general shape takes 37.32 s\n",
      "INFO 05-15 21:13:45 [monitor.py:33] torch.compile takes 45.82 s in total\n",
      "INFO 05-15 21:13:46 [kv_cache_utils.py:578] GPU KV cache size: 131,536 tokens\n",
      "INFO 05-15 21:13:46 [kv_cache_utils.py:581] Maximum concurrency for 32,768 tokens per request: 4.01x\n",
      "INFO 05-15 21:14:13 [gpu_model_runner.py:1608] Graph capturing finished in 27 secs, took 0.49 GiB\n",
      "INFO 05-15 21:14:13 [core.py:162] init engine (profile, create kv cache, warmup model) took 87.39 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/ssdshare/share/model/Qwen3-0.6B-Base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vLLM, generative models implement the VllmModelForTextGeneration interface. Based on the final hidden states of the input, these models output log probabilities of the tokens to generate, which are then passed through Sampler to obtain the final text.\n",
    "\n",
    "The `generate` method is available to all generative models in vLLM. It is similar to its counterpart in HF Transformers, except that tokenization and detokenization are also performed automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-15 21:14:22 [config.py:1088] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.85s/it, est. speed input: 2.08 toks/s, output: 184.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Which city is the capital of China?', Generated text: \" A. Wuhan B. Beijing C. Chengdu D. Chongqing\\nAnswer:\\nB\\n\\nSocial consciousness plays a significant role in social existence, bringing warmth to human society, promoting cultural development, enriching scientific and technological achievements, and increasing educational spending, which can cause symptoms such as inflation, food shortages, or labor shortages. Which of the following would NOT seriously affect social development? \\nA. A significant increase in social consciousness\\nB. A comparison between a 'social consciousness bewildering to the world' and 'a tongue always able to speak, a body always unhealthy'\\nC. The impact of actions that violate social morality\\nD. The potential long-term harm brought by twists and turns in the direction of social moral development\\nAnswer:\\nA D\\n\\nIn the large economy system, the 'powerfiends' referring to yuan is the only form of currency, while the 'officials' referring to People's Bank are the 'powerfollower' holders. For this 'twin relationship', Chinese officials should be familiar with 'some differences and necessary ways of handling relationships' to play the important role of intervening in the economy under the efforts of market regulation without interference by special agencies or vested interests. The reasons for the above statement include: ____ \\nA. Market rules are the foundation of social order\\nB. The allocation of market resource prices is the foundation of distribution according to work\\nC. The fluctuations of market prices are the foundation of economic fluctuations\\nD. The whole of the whole creates the part\\nAnswer:\\nC The explanation for this question involves options A, B, and D regarding the relationship between markets and their role in market order. The primary source of economic fluctuations is market prices; when the law of value dominates the economy, prices follow a relatively stable state, and markets achieve balance and fairness; disruption of the market order by the law of value represents the 'random' and 'lifetime' aspects of its conflict, lasting longer than the market order established by this law; thus, option C is incorrect and should be excluded. In contrast, option A stresses the importance of market rules regulating society, which is similar to the concept of the healthy social development; hence, it should be excluded because it does not involve compensating for unstable market fluctuations. Option B discusses the role of appropriate allocation regulating distribution according to work, quite similar to option D regarding the concept of cultivating a harmonious relationship, making both highly similar, therefore, option B and D should be excluded based on the options provided! Option D calls upon us to respect interconnections between the whole and its parts; the meaning of this phrase is not advantageous or unfavorable for determining a correct or incorrect answer. Therefore, option D should be excluded.\\n\\nGiven any two complex numbers a and b, when a + bi = 2 + i^2, then a equals ____ [1 point]\\nA. 1\\nB. 2 + i\\nC. 2 - i\\nD. 3 + i\\nAnswer:\\nA\\n\\nWhen did researchers first discover the mineral oxalate of nickel, and what is used as the raw material to produce nickel nitrite?\\nA. 1901 Gan Fan Judgement; Hydrochloric acid\\nB. 1852 Nobel Chronicles; Sodium hydroxide\\nC. 1897 Mr. Shu Zhang Remarks; Ferrous sulfate\\nD. 1787 Neptic Zhu Mǎi San Grand Theorem; Potassium hydroxide\\nAnswer:\\nC\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"Which city is the capital of China?\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally control the language generation by passing SamplingParams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 11.73 toks/s, output: 187.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Which city is the capital of China?', Generated text: ' Which province has the largest area in China?\\nA. Beijing, Heilongjiang Province\\nB. Shanghai, Heilongjiang Province\\nC. Beijing, Liaoning Province\\nD. Shanghai, Liaoning Province\\nAnswer: C\\n\\nWhich of the following options correctly represents the relationship of local standard to global standard?\\nA. Local standard > global standard\\nB. Local standard < global standard\\nC. Local standard = global standard\\nD. Unable to determine\\nAnswer: B\\n\\nWhen using a straight probe to detect the depth of a steel pipe, if the depth of the steel pipe is 100 millimeters,'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=128,\n",
    ")\n",
    "outputs = llm.generate(\"Which city is the capital of China?\", params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat method implements chat functionality on top of generate. In particular, it accepts input similar to OpenAI Chat Completions API and automatically applies the model’s chat template to format the prompt.\n",
    "\n",
    "In general, only instruction-tuned models have a chat template. Base models may perform poorly as they are not trained to respond to the chat conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 21:17:39 [config.py:209] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 05-15 21:17:48 [config.py:600] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-15 21:17:48 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-15 21:17:48 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/ssdshare/share/model/Phi-4-mini-instruct', speculative_config=None, tokenizer='/ssdshare/share/model/Phi-4-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/ssdshare/share/model/Phi-4-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-15 21:17:49 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f04f548a410>\n",
      "INFO 05-15 21:17:49 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-15 21:17:49 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 21:17:49 [gpu_model_runner.py:1258] Starting to load model /ssdshare/share/model/Phi-4-mini-instruct...\n",
      "WARNING 05-15 21:17:50 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dbe3116d334f149c9439490bbb5592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 21:17:51 [loader.py:447] Loading weights took 1.39 seconds\n",
      "INFO 05-15 21:17:51 [gpu_model_runner.py:1273] Model loading took 7.1694 GiB and 1.713549 seconds\n",
      "INFO 05-15 21:17:59 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/37d82d9555/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-15 21:17:59 [backends.py:426] Dynamo bytecode transform time: 7.13 s\n",
      "INFO 05-15 21:18:03 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 05-15 21:18:30 [backends.py:144] Compiling a graph for general shape takes 31.24 s\n",
      "INFO 05-15 21:18:47 [monitor.py:33] torch.compile takes 38.37 s in total\n",
      "INFO 05-15 21:18:48 [kv_cache_utils.py:578] GPU KV cache size: 26,464 tokens\n",
      "INFO 05-15 21:18:48 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 3.23x\n",
      "INFO 05-15 21:19:15 [gpu_model_runner.py:1608] Graph capturing finished in 27 secs, took 0.55 GiB\n",
      "INFO 05-15 21:19:15 [core.py:162] init engine (profile, create kv cache, warmup model) took 83.61 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import gc\n",
    "\n",
    "# terminate the previous LLM instance to free up memory\n",
    "llm = None\n",
    "gc.collect()\n",
    "llm = LLM(\n",
    "    model=\"/ssdshare/share/model/Phi-4-mini-instruct\",\n",
    "    max_model_len=8192,\n",
    "    max_num_seqs=1,\n",
    "    gpu_memory_utilization=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 21:19:19 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it, est. speed input: 5.97 toks/s, output: 99.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|system|>You are a helpful assistant<|end|><|user|>Hello<|end|><|assistant|>Hello! How can I assist you today?<|end|><|user|>Write an long essay about the importance of higher education.<|end|><|assistant|>',\n",
      "\n",
      "Generated text: \"Title: The Imperative of Higher Education in Contemporary Society\\n\\nIn the ever-evolving landscape of contemporary society, the significance of higher education has never been more pronounced. Higher education serves as a cornerstone of personal development, economic advancement, and societal progress, offering myriad benefits that extend far beyond the confines of academia. This essay elucidates the multifaceted importance of higher education, exploring its impact on individuals, economies, and communities at large.\\n\\nAt the individual level, higher education serves as a catalyst for personal growth and intellectual development. The pursuit of higher education exposes students to diverse disciplines, fostering critical thinking, analytical skills, and creativity. The process of engaging with complex concepts and theories cultivates a deeper understanding of the world, enabling individuals to navigate life's challenges with greater resilience and adaptability. Moreover, higher education provides a platform for students to cultivate essential soft skills such as communication, leadership, teamwork, and problem-solving, which are indispensable in both personal and professional realms.\\n\\nHigher education also plays a pivotal role in fostering social mobility, breaking down barriers that perpetuate inequality. Access to quality education equips individuals from diverse backgrounds with the knowledge and skills necessary to improve their socioeconomic status. By leveling the playing field, higher education serves as a vehicle for individuals to transcend their circumstances and achieve their aspirations. Furthermore, the opportunities for networking and mentorship that higher education institutions offer can significantly impact an individual's career trajectory, opening doors to new possibilities and avenues for growth.\\n\\nFrom an economic standpoint, higher education is an indispensable driver of economic growth and innovation. In today's globalized world, economies increasingly rely on a highly skilled workforce to remain competitive. Higher education institutions, through their research and development initiatives, contribute to the creation of cutting-edge technologies, novel solutions to societal problems, and advancements in various fields. Moreover, the graduates of higher education institutions constitute a significant portion of the workforce, bringing with them a wealth of knowledge, expertise, and innovation that propels industries forward.\\n\\nHigher education is also instrumental in addressing and mitigating societal challenges. In an era marked by complex and multifaceted issues such as climate change, public health crises, and social injustice, higher education institutions play a crucial role in fostering interdisciplinary collaboration and innovation. The convergence of diverse perspectives and expertise within academia enables researchers and scholars to devise holistic solutions that address these pressing concerns. Furthermore, higher education institutions serve as hubs of civic engagement, empowering students to actively participate in the democratic process and contribute to the betterment of their communities.\\n\\nIn conclusion, the importance of higher education cannot be overstated. As a catalyst for personal growth, a vehicle for social mobility, an engine of economic growth, and a beacon of societal progress, higher education is an indispensable component of contemporary society. As we navigate the complexities of the 21st century, it is imperative that we recognize and uphold the value of higher education, investing in its accessibility, quality, and relevance to ensure a brighter future for generations to come.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an long essay about the importance of higher education.\",\n",
    "    },\n",
    "]\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "outputs = llm.chat(conversation, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r},\\n\\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start the server via the vllm serve command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it in your terminal\n",
    "# vllm serve /ssdshare/share/model/Phi-4-mini-instruct --dtype auto --api-key token-abc123 --max-model-len 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use OpenAI python package to access the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/ssdshare/share/model/Phi-4-mini-instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vllm provides a set of endpoints that are compatible with OpenAI API, like completion, chat completion, embedding, and so on. You can find the full list of endpoints in the vllm documentation.\n",
    "\n",
    "Moreover, vllm also provides a set of metrics endpoints that can be used to monitor the state and performance of the server.\n",
    "Some of the metrics are: TTFT, TPOT.\n",
    "\n",
    "TTFT is the time it takes to generate the first token of the response. TPOT is the time it takes to generate each token of the response. These metrics are so-called SLO (Service Level Objective) metrics, which are used to measure the performance of the server. VLLM did a lot of work to optimize these SLO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Adding a Web User Interface using `gradio`\n",
    "\n",
    "Demo a machine learning application is important. It gives the users a direct experience of your algorithm in an interactive manner. Here we'll be building an interesting demo using `gradio`, a popular Python library for ML demos. Let's install this library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% pip install gradio --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are able to write an example UI that takes in a text string and output a processed string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/gradio_example.py\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, hello \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the gradio server by runnning the following command\n",
    "\n",
    "# python /tmp/gradio_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "\n",
    "## Try change the last line (launch) to \n",
    "\n",
    "## demo.launch(share=True) \n",
    "## observe the output and see the link to open (without the need of port forwarding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The ChatInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/gradio_example.py\n",
    "\n",
    "import random\n",
    "\n",
    "def random_response(message, history):\n",
    "    return random.choice([\"Yes\", \"No\"])\n",
    "\n",
    "import gradio as gr\n",
    "gr.ChatInterface(random_response).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill your previous process, and restart the new process\n",
    "\n",
    "# python /tmp/gradio_example.py\n",
    "\n",
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Quick and dirty way of creating a UI for a HuggingFace pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/simpleui.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/simpleui.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gradio as gr\n",
    "\n",
    "model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ") \n",
    "gr.Interface.from_pipeline(pipe).launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python /tmp/simpleui.py\n",
    "\n",
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 A better way to build a web UI for LLM (through an LLM API server)\n",
    "\n",
    "Next, you should implement a script that interact with the Phi-4-mini Chat API server you just created.  \n",
    "\n",
    "Note that you should directly call the API server using request, instead of running the LLM within your UI server process. \n",
    "\n",
    "![Illustration of request](./assets/request.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/llm_api.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/llm_api.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "         \n",
    "from fastapi import FastAPI, Request, Query\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.6,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n",
    "    else:\n",
    "        messages = history\n",
    "    if user_prompt:\n",
    "        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        messages.extend(prompt_msg)\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output\n",
    "\n",
    "class Input(BaseModel):\n",
    "    Prompt: str\n",
    "    History: list\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def run_post(input: Input):\n",
    "    output = chat_resp(model, tokenizer, user_prompt=input.Prompt, history=input.History)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True,\n",
    "                                             )\n",
    "    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/chatUI.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/chatUI.py\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def predict(message, history):\n",
    "    url = \"http://localhost:54223/chat\"\n",
    "    history = json.loads(history)\n",
    "    data = {\n",
    "        \"Prompt\": message,\n",
    "        \"History\": history\n",
    "    }\n",
    "    encoded = json.dumps(data).encode(\"utf-8\")\n",
    "    response = requests.post(url, data = encoded)\n",
    "    return response.text\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\"text\", \"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "#### Your Task ####\n",
    "# Insert code here to perform the inference\n",
    "# You can use either the hand-crafted API server or the OpenAI-compatible vLLM server\n",
    "# [{\"role\": \"user\", \"content\": \"My name is Tiger.\"}]\n",
    "#### End Task ####\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not forget to start your API server (from above, use the /chat API or use the vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/chatUI.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/chatUI.py\n",
    "# vllm serve /ssdshare/share/model/Phi-4-mini-instruct --dtype auto --api-key token-abc123 --max-model-len 16384\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\"\n",
    ")\n",
    "\n",
    "def predict(message, history):\n",
    "    history = json.loads(history)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"/ssdshare/share/model/Phi-4-mini-instruct\",\n",
    "        messages=history + [{\"role\": \"user\", \"content\": message}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\"text\", \"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 More Gradio: Streaming and Multi-media\n",
    "\n",
    "Gradio also supports streaming and multi-media input and output.\n",
    "\n",
    "Magic happens from the `streaming=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/transcribe.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/transcribe.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_path = \"/ssdshare/share/model/whisper-large-v3-turbo\" # a multi-lingual audio transcription model\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "def transcribe(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if y.ndim > 1:\n",
    "        y = y.mean(axis=1)\n",
    "\n",
    "    # normalize\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    return stream, pipe(\n",
    "        {\n",
    "            \"sampling_rate\": sr,\n",
    "            \"raw\": stream,\n",
    "            \"return_timestamps\": True,\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"chinese\",\n",
    "        }\n",
    "    )[\"text\"]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)], # note the streaming=True\n",
    "    [\"state\", \"text\"],\n",
    "    live=True,\n",
    "    time_limit=30,\n",
    "    stream_every=0.5,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python /tmp/transcribe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Build your own Gradio UI\n",
    "\n",
    "Create a separate Gradio UI to serve other models. Maybe an image model in Lab 5, or a translation model cooperated with a transcription model? Explore the Gradio documentation and HuggingFace model cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "Could not create share link. Missing file: /usr/local/lib/python3.10/dist-packages/gradio/frpc_linux_amd64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.3\n",
      "3. Move the file to this location: /usr/local/lib/python3.10/dist-packages/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Your Task ####\n",
    "#### End Task ####\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "base_url = os.getenv(\"SILICON_BASE_URL\")\n",
    "api_key = os.getenv(\"SILICON_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def image_generation(prompt):\n",
    "    response = client.images.generate(\n",
    "        model=\"Kwai-Kolors/Kolors\",\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    # Extract the image URL from the response\n",
    "    image_url = response.data[0].url\n",
    "    return image_url\n",
    "\n",
    "gr.Interface(\n",
    "    fn=image_generation,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=gr.Image(type=\"filepath\", label=\"Generated Image\"),\n",
    ").launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
