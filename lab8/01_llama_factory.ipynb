{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 Supervised Fine Tuning\n",
    "\n",
    "In this lab, we will perform parameter efficient finetuning (PEFT) to finetune a llama-2 model, using the HuggingFace SFTTrainer tool from its trl library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add proxy to access openai ...\n",
    "import os\n",
    "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\"\n",
    "os.environ['HTTP_PROXY']=\"\"\n",
    "os.environ['HTTPS_PROXY']=\"\"\n",
    "os.environ['ALL_PROXY']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /trl/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting trl (from -r requirements.txt (line 1))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/da/f2/6f47dd96314a281b45695da75e28ece3a9b55931f965587767fc374492a1/trl-0.17.0-py3-none-any.whl (348 kB)\n",
      "Collecting fire (from -r requirements.txt (line 2))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/6b/b6/82c7e601d6d3c3278c40b7bd35e17e82aa227f050aa9f66cb7b7fce29471/fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.45.5)\n",
      "Collecting deepspeed (from -r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/06/b3/a3903de5c5b707170c5c27e1a40f4ef613f14d241bd84d8b151a2a8786f6/deepspeed-0.16.7.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (4.51.0)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 2))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (0.8.0)\n",
      "Collecting hjson (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.11.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/db/24/552ebea28f0570b9e65e62b50287a273804c9f997cc1c2dcd4e2d64b9e7d/nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: fire, deepspeed\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=fa5fc00144d14bcbde1af8f9e63bb6ef244e1df9b97f55cb6fc9bccb1f3593d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lm98vq9e/wheels/c9/11/74/727f4d8a518ce4b73cf7c0134249b9287c9d5fac9531853b0f\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.16.7-py3-none-any.whl size=1642805 sha256=61b143bff1b66072478ca7336e6698215a60b399081116cf139a41de9f4049e2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lm98vq9e/wheels/3f/59/52/2678b0ad7418b0838b260c56b2393cfebf5554482e144b68d9\n",
      "Successfully built fire deepspeed\n",
      "Installing collected packages: nvidia-ml-py, hjson, termcolor, fire, deepspeed, trl\n",
      "Successfully installed deepspeed-0.16.7 fire-0.7.0 hjson-3.1.0 nvidia-ml-py-12.575.51 termcolor-3.1.0 trl-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %cd /root/llm_course_public/lab8\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "# !pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "#!mkdir -p /root/LLM-applications-course/lab8/LLaMA-Factory\n",
    "#!cd /root/LLM-applications-course/lab8/LLaMA-Factory/ && pip install -r /root/LLM-applications-course/lab8/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first change the working directory to /gfshome, to avoid writing too much data to the home directory. (Ignore the warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/gfshome/Llama3-8B-Instruct-sft.yaml': File exists\n",
      "ln: failed to create symbolic link '/gfshome/Lora_Merge.yaml': File exists\n"
     ]
    }
   ],
   "source": [
    "# copy the config files to /gfshome, the working directory (will need later)\n",
    "!ln -s *.yaml /gfshome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gfshome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gfshome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///gfshome/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (4.51.0)\n",
      "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.23.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.14.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.115.9)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/c8/48/3e49cf0f64961656402c0023edbc51844fe17afe53ab50e958a6dbbbd499/sse_starlette-2.3.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.9.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/23/42/0eafe0de75de6a0db71add8e4ea51ebf090482bad3068f4a874c90fbd110/av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.1)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/60/ec/e34d546cfd9c5b906d1d534bb75557be9f2b179609d60bb9e97ec07e8ead/tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Collecting jieba (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.10.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.6.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.23.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.9.11)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.0.8)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/3e/38/7859ff46355f76f8d19459005ca000b6e7012f2f1ca597746cbcd1fbfe5e/antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.16.0)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.tuna.tsinghua.edu.cn/pypi/web/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->llamafactory==0.9.3.dev0) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Building wheels for collected packages: llamafactory, jieba, antlr4-python3-runtime\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27215 sha256=d2e2b428e321f6a8e490f8433f3e2a6687afd7cf0176a54f58162497694a6c1b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wc2sftnv/wheels/87/26/82/8f4922c9e797dfc3e05b24c481d0e498ffae7c1e700eb2c667\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=5ef5ea94628863ae18ba09a50eac77937b8bc9aa6a9958848f66ef9b2ddd29a6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wc2sftnv/wheels/c3/6e/b3/e6eeda505169541338a7a78c25eda3044d6d555265b5a96bad\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144551 sha256=150b783e9a72e494abb44622c1ee22061de8ad9ffa8c91e18b398129335717ff\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wc2sftnv/wheels/95/92/e2/0fdd74e0936fc7fbefa7325fc9fdaf73bce9a7496c303c98a7\n",
      "Successfully built llamafactory jieba antlr4-python3-runtime\n",
      "Installing collected packages: jieba, antlr4-python3-runtime, shtab, rouge-chinese, omegaconf, docstring-parser, av, anyio, tyro, sse-starlette, trl, llamafactory\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.0\n",
      "    Uninstalling anyio-4.6.0:\n",
      "      Successfully uninstalled anyio-4.6.0\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.17.0\n",
      "    Uninstalling trl-0.17.0:\n",
      "      Successfully uninstalled trl-0.17.0\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 anyio-4.9.0 av-14.4.0 docstring-parser-0.16 jieba-0.42.1 llamafactory-0.9.3.dev0 omegaconf-2.3.0 rouge-chinese-1.0.3 shtab-1.7.2 sse-starlette-2.3.5 trl-0.9.6 tyro-0.8.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#download llama factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory && pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Supervised Fine Tuning Example\n",
    "### 2.1 Motivation\n",
    "Llama3 is a versatile large language model available in various parameter sizes. Given its significant improvements in text generation tasks compared to its predecessor, Llama2, we aim to use Llama3-8B-Instruct to generate Chinese poetry based on specific themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Shared parameters between inference and SFT training\n",
    "################################################################################\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# The base model\n",
    "model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# Use a single GPU\n",
    "# device_map = {'':0}\n",
    "# Use all GPUs\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fced0f9f2eb8439784a1077c00312c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]</s>\n",
      "\n",
      "**Translation:**\n",
      "\"Travelers brave the wind and rain,\n",
      "Seeking a glimpse of the distant land.\"\n",
      "\n",
      "**Original Poem:**\n",
      "(风雨旅人)\n",
      "(寻远方)\n",
      "\n",
      "**Explanation:**\n",
      "The poem is written in the traditional Chinese poetic form, with a 5-character structure. The poem captures the theme of 风雨，旅人, which is about travelers facing the challenges of wind and rain while on their journey. The poem uses simple and concise language to convey the idea of perseverance and determination in the face of adversity. The first line, (风雨旅人), sets the scene, while the second line, (寻远方), expresses the traveler's longing to reach their destination. The poem is meant to be a reflection of the human spirit's ability to overcome obstacles and push forward, even in the most challenging of circumstances.\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") \n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output does not make any sense. Not only the number of characters in each line is not suffcient to our requirement, but also the tune and words used is not like ancient poet at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the training dataset\n",
    "\n",
    "Let's use sft to improve Llama3-8B-Instruct's ablity in this field now!\n",
    "\n",
    "You should prepare for the data we need to use for SFT in `02_poet data` .\n",
    "\n",
    "Please complete the procedures in that notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SFT with Llama-Factory\n",
    "\n",
    "For Processing SFT, we use llama factory, which is a highly modular, user-friendly platform with great ease of use, supporting distributed training and a variety of pre-trained models. Llama factory provide a WebUI to make it easy for using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-22 21:20:56,581] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-22 21:21:02 [__init__.py:239] Automatically detected platform cuda.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "[2025-05-22 21:21:47,867] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-22 21:21:53 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-22 21:21:57] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:35985\n",
      "W0522 21:21:59.133000 13466 torch/distributed/run.py:792] \n",
      "W0522 21:21:59.133000 13466 torch/distributed/run.py:792] *****************************************\n",
      "W0522 21:21:59.133000 13466 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0522 21:21:59.133000 13466 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-22 21:22:04,880] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:22:04,881] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:22:04,924] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:22:04,959] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-22 21:22:11,016] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:22:11,071] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:22:11,072] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:22:11,072] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-22 21:22:11,129] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[WARNING|2025-05-22 21:22:11] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-22 21:22:11] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-22 21:22:11] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-22 21:22:11] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,796 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,797 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,797 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,797 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,797 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:11,798 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-22 21:22:11] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-22 21:22:11] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-22 21:22:12,297 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:22:12,302 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:22:12,303 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,311 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,311 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,311 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,312 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,312 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:22:12,312 >> loading file chat_template.jinja\n",
      "[rank2]:[W522 21:22:12.949958535 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-22 21:22:12,749 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[rank1]:[W522 21:22:12.984106178 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|2025-05-22 21:22:12] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-22 21:22:12] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-22 21:22:12] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-22 21:22:12] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2025-05-22 21:22:12] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "[rank3]:[W522 21:22:12.038224213 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W522 21:22:13.206317414 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 439, 264, 8620, 40360, 11, 649, 499, 1520, 757, 311, 1893, 264, 220, 20, 80325, 220, 19, 8614, 33894, 430, 52924, 279, 22100, 315, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, as a Chinese poet, can you help me to create a 5-character 4-line poem that incorporates the themes of 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:22:15,389 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:22:15,390 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-22 21:22:15] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-22 21:22:15] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-22 21:22:15,529 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-22 21:22:15,529 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-22 21:22:15,531 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:14<00:00,  3.52s/it]\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:14<00:00,  3.58s/it]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-22 21:22:30,017 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-22 21:22:30,018 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-22 21:22:30,022 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-22 21:22:30,023 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-22 21:22:30] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-22 21:22:30] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-22 21:22:30] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-22 21:22:30] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-22 21:22:30] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,down_proj,v_proj,gate_proj,o_proj,q_proj,k_proj\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "[INFO|2025-05-22 21:22:33] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-22 21:22:33,836 >> Using auto half precision backend\n",
      "[2025-05-22 21:22:34,156] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.8, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-22 21:22:34,156] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-22 21:22:34,755] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-22 21:22:34,761] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-22 21:22:34,761] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-22 21:22:34,806] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-22 21:22:34,806] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-22 21:22:34,806] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-22 21:22:34,806] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-22 21:22:34,806] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-22 21:22:34,806] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-22 21:22:34,806] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-22 21:22:34,806] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 96, in run_sft\n",
      "[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "[rank0]:     return inner_training_loop(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2374, in _inner_training_loop\n",
      "[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1440, in prepare\n",
      "[rank0]:     result = self._prepare_deepspeed(*args)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2033, in _prepare_deepspeed\n",
      "[rank0]:     engine, optimizer, _, lr_scheduler = ds_initialize(**kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/__init__.py\", line 193, in initialize\n",
      "[rank0]:     engine = DeepSpeedEngine(args=args,\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 326, in __init__\n",
      "[rank0]:     self._configure_optimizer(optimizer, model_parameters)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1408, in _configure_optimizer\n",
      "[rank0]:     self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1666, in _configure_zero_optimizer\n",
      "[rank0]:     optimizer = DeepSpeedZeroOptimizer(\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 365, in __init__\n",
      "[rank0]:     self.bit16_groups_flat.append(flattened_buffer.to(get_accelerator().current_device_name()))\n",
      "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 318.06 MiB is free. Process 1764999 has 16.89 GiB memory in use. Process 1765001 has 384.00 MiB memory in use. Process 1793811 has 5.95 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 69.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W522 21:22:36.942383573 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0522 21:22:38.639000 13466 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13542 closing signal SIGTERM\n",
      "W0522 21:22:38.640000 13466 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13543 closing signal SIGTERM\n",
      "W0522 21:22:38.641000 13466 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13544 closing signal SIGTERM\n",
      "E0522 21:22:39.220000 13466 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 13541) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/gfshome/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-22_21:22:38\n",
      "  host      : s1313-pytorch-hfjlab8-6569776fbd-2hbzw\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 13541)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 95, in main\n",
      "    process = subprocess.run(\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '35985', '/gfshome/LLaMA-Factory/src/llamafactory/launcher.py', 'saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/training_args.yaml']' returned non-zero exit status 1.\n",
      "[2025-05-22 21:25:01,428] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-22 21:25:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-22 21:25:11] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:35843\n",
      "W0522 21:25:13.045000 14740 torch/distributed/run.py:792] \n",
      "W0522 21:25:13.045000 14740 torch/distributed/run.py:792] *****************************************\n",
      "W0522 21:25:13.045000 14740 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0522 21:25:13.045000 14740 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-22 21:25:18,800] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:25:18,801] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:25:18,846] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-22 21:25:18,851] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-22 21:25:24,874] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:25:24,875] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:25:24,895] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:25:24,925] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-22 21:25:24,925] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[WARNING|2025-05-22 21:25:25] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-22 21:25:25] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-22 21:25:25] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,397 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,397 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,398 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,398 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,398 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,398 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-22 21:25:25] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-22 21:25:25] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-22 21:25:25] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-22 21:25:25,889 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:25:25,898 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:25:25,900 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-22 21:25:25,908 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-22 21:25:26,337 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-22 21:25:26] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-22 21:25:26] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-22 21:25:26] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-22 21:25:26] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2025-05-22 21:25:26] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "[rank3]:[W522 21:25:26.614443759 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W522 21:25:26.650757354 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank2]:[W522 21:25:26.683347053 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W522 21:25:27.635019334 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 439, 264, 8620, 40360, 11, 649, 499, 1520, 757, 311, 1893, 264, 220, 20, 80325, 220, 19, 8614, 33894, 430, 52924, 279, 22100, 315, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, as a Chinese poet, can you help me to create a 5-character 4-line poem that incorporates the themes of 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:25:28,771 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:25:28,772 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-22 21:25:28] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-22 21:25:28] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-22 21:25:28,913 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-22 21:25:28,914 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-22 21:25:28,916 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:14<00:00,  3.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:18<00:00,  4.53s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:18<00:00,  4.57s/it]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-22 21:25:47,363 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-22 21:25:47,364 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-22 21:25:47,371 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-22 21:25:47,371 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-22 21:25:47] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-22 21:25:47] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-22 21:25:47] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-22 21:25:47] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-22 21:25:47] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,gate_proj,k_proj,down_proj,up_proj,o_proj,v_proj\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:18<00:00,  4.65s/it]\n",
      "[INFO|2025-05-22 21:25:49] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-22 21:25:49,427 >> Using auto half precision backend\n",
      "[2025-05-22 21:25:49,775] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.8, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-22 21:25:49,775] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-22 21:25:50,397] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-22 21:25:50,435] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-22 21:25:50,435] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-22 21:25:50,497] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-22 21:25:50,498] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-22 21:25:50,498] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-22 21:25:50,498] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-22 21:25:50,498] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-22 21:25:50,498] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-22 21:25:50,498] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-22 21:25:50,498] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[2025-05-22 21:25:51,943] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-22 21:25:51,944] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.86 GB         CA 5.93 GB         Max_CA 6 GB \n",
      "[2025-05-22 21:25:51,944] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.87 GB, percent = 4.0%\n",
      "[2025-05-22 21:25:52,165] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-22 21:25:52,165] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.94 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-22 21:25:52,166] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.88 GB, percent = 4.0%\n",
      "[2025-05-22 21:25:52,166] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n",
      "[2025-05-22 21:25:52,382] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-22 21:25:52,382] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.78 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-22 21:25:52,383] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.87 GB, percent = 4.0%\n",
      "[2025-05-22 21:25:52,386] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-22 21:25:52,386] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-22 21:25:52,386] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-22 21:25:52,386] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-05-22 21:25:52,394] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-22 21:25:52,394] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-22 21:25:52,394] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-22 21:25:52,394] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-22 21:25:52,394] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efbb82c4a30>\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-22 21:25:52,395] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.3\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   train_batch_size ............. 128\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  32\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-22 21:25:52,396] [INFO] [config.py:1007:print]   world_size ................... 4\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n",
      "[2025-05-22 21:25:52,397] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 0.3, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"round_robin_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:2414] 2025-05-22 21:25:52,398 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-22 21:25:52,398 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-22 21:25:52,398 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-05-22 21:25:52,398 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2420] 2025-05-22 21:25:52,399 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:2421] 2025-05-22 21:25:52,399 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-22 21:25:52,399 >>   Total optimization steps = 782\n",
      "[INFO|trainer.py:2423] 2025-05-22 21:25:52,403 >>   Number of trainable parameters = 167,772,160\n",
      " 13%|█████▏                                   | 100/782 [02:37<18:00,  1.58s/it][INFO|2025-05-22 21:28:29] llamafactory.train.callbacks:143 >> {'loss': 3.7611, 'learning_rate': 9.9000e-05, 'epoch': 0.13, 'throughput': 7855.30}\n",
      "{'loss': 3.7611, 'grad_norm': 0.39261528849601746, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.13, 'num_input_tokens_seen': 1235712}\n",
      " 26%|██████████▍                              | 200/782 [05:15<15:15,  1.57s/it][INFO|2025-05-22 21:31:07] llamafactory.train.callbacks:143 >> {'loss': 3.1686, 'learning_rate': 1.9900e-04, 'epoch': 0.26, 'throughput': 7842.49}\n",
      "{'loss': 3.1686, 'grad_norm': 0.4008963704109192, 'learning_rate': 0.000199, 'epoch': 0.26, 'num_input_tokens_seen': 2472960}\n",
      " 26%|██████████▍                              | 200/782 [05:15<15:15,  1.57s/it][INFO|trainer.py:3984] 2025-05-22 21:31:12,677 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:31:12,750 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:31:12,751 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-22 21:31:15,923 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-22 21:31:15,933 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/special_tokens_map.json\n",
      "[2025-05-22 21:31:16,721] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2025-05-22 21:31:16,831] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/mp_rank_00_model_states.pt\n",
      "[2025-05-22 21:31:16,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/mp_rank_00_model_states.pt...\n",
      "[2025-05-22 21:31:21,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/mp_rank_00_model_states.pt.\n",
      "[2025-05-22 21:31:21,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-22 21:31:28,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-22 21:31:28,691] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-22 21:31:28,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n",
      " 38%|███████████████▋                         | 300/782 [08:14<12:51,  1.60s/it][INFO|2025-05-22 21:34:06] llamafactory.train.callbacks:143 >> {'loss': 3.0318, 'learning_rate': 1.8606e-04, 'epoch': 0.38, 'throughput': 7505.06}\n",
      "{'loss': 3.0318, 'grad_norm': 0.3564233183860779, 'learning_rate': 0.00018605771158039253, 'epoch': 0.38, 'num_input_tokens_seen': 3711232}\n",
      " 51%|████████████████████▉                    | 400/782 [10:51<09:59,  1.57s/it][INFO|2025-05-22 21:36:44] llamafactory.train.callbacks:143 >> {'loss': 2.9326, 'learning_rate': 1.4764e-04, 'epoch': 0.51, 'throughput': 7591.02}\n",
      "{'loss': 2.9326, 'grad_norm': 0.3598722517490387, 'learning_rate': 0.00014764470355274875, 'epoch': 0.51, 'num_input_tokens_seen': 4947712}\n",
      " 51%|████████████████████▉                    | 400/782 [10:51<09:59,  1.57s/it][INFO|trainer.py:3984] 2025-05-22 21:36:48,776 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:36:48,875 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:36:48,878 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-22 21:36:52,017 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-22 21:36:52,034 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/special_tokens_map.json\n",
      "[2025-05-22 21:36:52,791] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!\n",
      "[2025-05-22 21:36:52,870] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/mp_rank_00_model_states.pt\n",
      "[2025-05-22 21:36:52,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/mp_rank_00_model_states.pt...\n",
      "[2025-05-22 21:36:57,391] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/mp_rank_00_model_states.pt.\n",
      "[2025-05-22 21:36:57,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-22 21:37:04,472] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-22 21:37:04,495] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-22 21:37:04,495] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!\n",
      " 64%|██████████████████████████▏              | 500/782 [13:49<07:28,  1.59s/it][INFO|2025-05-22 21:39:41] llamafactory.train.callbacks:143 >> {'loss': 2.8511, 'learning_rate': 9.5683e-05, 'epoch': 0.64, 'throughput': 7454.88}\n",
      "{'loss': 2.8511, 'grad_norm': 0.33802586793899536, 'learning_rate': 9.568300160236304e-05, 'epoch': 0.64, 'num_input_tokens_seen': 6183424}\n",
      " 77%|███████████████████████████████▍         | 600/782 [16:27<04:45,  1.57s/it][INFO|2025-05-22 21:42:19] llamafactory.train.callbacks:143 >> {'loss': 2.7970, 'learning_rate': 4.4949e-05, 'epoch': 0.77, 'throughput': 7516.63}\n",
      "{'loss': 2.797, 'grad_norm': 0.34656885266304016, 'learning_rate': 4.494892172941965e-05, 'epoch': 0.77, 'num_input_tokens_seen': 7421184}\n",
      " 77%|███████████████████████████████▍         | 600/782 [16:27<04:45,  1.57s/it][INFO|trainer.py:3984] 2025-05-22 21:42:24,300 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:42:24,401 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:42:24,403 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-22 21:42:27,510 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-22 21:42:27,519 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/special_tokens_map.json\n",
      "[2025-05-22 21:42:28,281] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!\n",
      "[2025-05-22 21:42:28,354] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/mp_rank_00_model_states.pt\n",
      "[2025-05-22 21:42:28,354] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/mp_rank_00_model_states.pt...\n",
      "[2025-05-22 21:42:33,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/mp_rank_00_model_states.pt.\n",
      "[2025-05-22 21:42:33,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-22 21:42:40,886] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-22 21:42:40,967] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-22 21:42:40,967] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!\n",
      " 90%|████████████████████████████████████▋    | 700/782 [19:26<02:07,  1.56s/it][INFO|2025-05-22 21:45:19] llamafactory.train.callbacks:143 >> {'loss': 2.7575, 'learning_rate': 9.8697e-06, 'epoch': 0.90, 'throughput': 7422.47}\n",
      "{'loss': 2.7575, 'grad_norm': 0.3498210906982422, 'learning_rate': 9.869681827916777e-06, 'epoch': 0.9, 'num_input_tokens_seen': 8659456}\n",
      "100%|█████████████████████████████████████████| 782/782 [21:36<00:00,  1.57s/it][INFO|trainer.py:3984] 2025-05-22 21:47:33,670 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:47:33,729 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:47:33,730 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-22 21:47:37,203 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-22 21:47:37,213 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/special_tokens_map.json\n",
      "[2025-05-22 21:47:37,974] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step782 is about to be saved!\n",
      "[2025-05-22 21:47:38,072] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/mp_rank_00_model_states.pt\n",
      "[2025-05-22 21:47:38,072] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/mp_rank_00_model_states.pt...\n",
      "[2025-05-22 21:47:43,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/mp_rank_00_model_states.pt.\n",
      "[2025-05-22 21:47:43,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-22 21:47:49,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-22 21:47:49,990] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/checkpoint-782/global_step782/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-22 21:47:49,991] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step782 is ready now!\n",
      "[INFO|trainer.py:2681] 2025-05-22 21:47:50,164 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1317.7612, 'train_samples_per_second': 75.886, 'train_steps_per_second': 0.593, 'train_loss': 3.012483933392693, 'epoch': 1.0, 'num_input_tokens_seen': 9676544}\n",
      "100%|█████████████████████████████████████████| 782/782 [21:57<00:00,  1.69s/it]\n",
      "[INFO|trainer.py:3984] 2025-05-22 21:47:54,972 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11\n",
      "[INFO|configuration_utils.py:691] 2025-05-22 21:47:55,058 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-22 21:47:55,060 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-22 21:47:58,066 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-22 21:47:58,076 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =         1.0\n",
      "  num_input_tokens_seen    =     9676544\n",
      "  total_flos               = 414877320GF\n",
      "  train_loss               =      3.0125\n",
      "  train_runtime            =  0:21:57.76\n",
      "  train_samples_per_second =      75.886\n",
      "  train_steps_per_second   =       0.593\n",
      "Figure saved at: saves/Llama-3-8B-Instruct/lora/train_2025-05-22-21-21-11/training_loss.png\n",
      "[WARNING|2025-05-22 21:47:59] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-22 21:47:59] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-22 21:47:59,064 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "Keyboard interruption in main thread... closing server.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2903, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "!cd LLaMA-Factory && llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the training parameters we selected in the file `Llama3-8B-Instruct-sft.yaml`, or refer to the screenshot in the slides. After you fullfill the parameters, click `Start` and wait for the SFT process to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training runs to complete, please paste your loss change chat below. \n",
    "\n",
    "![The Loss Figure](loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2025-05-18-10-33-31  train_2025-05-22-20-58-25  train_2025-05-22-21-21-11\n"
     ]
    }
   ],
   "source": [
    "# You can now terminate the training process by stopping the previous cell.\n",
    "# The resulting LoRA is saved in LLaMA-Factory/saves/Llama-3-8B-Instruct/lora \n",
    "# (who is automatically named with a date as suffix)\n",
    "!ls LLaMA-Factory/saves/Llama-3-8B-Instruct/lora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the LoRA into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 11:13:07,840] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 11:13:12 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,741 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,742 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,742 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,742 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,742 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:16,742 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 11:13:17,179 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 11:13:17,188 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 11:13:17,190 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:13:17,198 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 11:13:17,581 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-18 11:13:17] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-18 11:13:17] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-18 11:13:17] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-18 11:13:17] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 11:13:17,601 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 11:13:17,602 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 11:13:17] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1121] 2025-05-18 11:13:17,675 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-18 11:13:17,675 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 11:13:17,699 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 42.58it/s]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-18 11:13:17,830 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-18 11:13:17,830 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-18 11:13:17,835 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 11:13:17,835 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 11:13:17] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|2025-05-18 11:14:44] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-18 11:14:44] llamafactory.model.adapter:143 >> Loaded adapter(s): /gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-33-31\n",
      "[INFO|2025-05-18 11:14:44] llamafactory.model.loader:143 >> all params: 8,030,261,248\n",
      "[INFO|2025-05-18 11:14:44] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:419] 2025-05-18 11:14:44,921 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-18 11:14:44,931 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-05-18 11:17:01,814 >> The model is bigger than the maximum size per checkpoint (4GB) and is going to be split in 5 checkpoint shards. You can find where each parameters has been saved in the index located at /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 11:17:01,828 >> tokenizer config file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 11:17:01,836 >> Special tokens file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/special_tokens_map.json\n",
      "[INFO|2025-05-18 11:17:02] llamafactory.train.tuner:143 >> Ollama modelfile saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Merge Lora_model with Base model and save the merged model\n",
    "# ***Update the Lora-Merge.yaml configuration file and fullfill the Lora Path***\n",
    "# For more options in export, please refer to the [Llama-Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/export.md)\n",
    "\n",
    "!llamafactory-cli export Lora_Merge.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose your Finetuneed model for test\n",
    "#Dont't forget to change the model name to your export_dir\n",
    "model_name = \"/gfshome/merged_model/Llama-3-8B-Instruct-sft-poet\"  # your new model \n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 11:19:35,280] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d3cdd09e9241349ca61045c4e996ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you don't want to merge your lora to get a new model, you can just using the lora when inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# device_map = \"auto\"\n",
    "# adapter_name_or_path = \"/gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-09-25-23\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "#     bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "#     bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "#     bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    "# )\n",
    "\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     pipeline,\n",
    "# )\n",
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     adapter_name_or_path, \n",
    "#     device_map=device_map\n",
    "# )\n",
    "\n",
    "# os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]旅人心最苦，\n",
      "风雨不可期。[/INST]\n",
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of旅人[/INST]旅人心最苦，\n",
      "不见故乡山。[/INST]\n",
      "<s\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源. [/INST]桃源无路可寻处，\n",
      "可惜花开不用心。\n",
      "为问桃源何处是，\n",
      "可怜花在眼前深。[/INST]\n",
      "<s>[INST] Hi, please draft a 7-character 4-line chinese ancient poem based on the themes:桃源[/INST\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源.\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税? [/INST]>\n",
      "\n",
      "美国关税总要缴，\n",
      "老夫一病不能逃。\n",
      "不知老去多多少，\n",
      "每月何如十万毫。[/INST] <s>[INST] Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # 如果 tokenizer 支持这个 token\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
